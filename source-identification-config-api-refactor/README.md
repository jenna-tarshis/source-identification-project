This README has **two major sections**:

1. **Anomaly Detection (EVA & dynamic thresholding)** — event detection.
2. **Spatiotemporal Source Localization** — event origin estimation.

---

# PART 1 — Anomaly Detection & Dynamic Thresholding

This part of the project implements **Extreme Value Analysis (EVA)**, which is a way to calculate a dynamic threshold to detect anomalous measurements in time series data. The user specifies a time frame and the program colects block maxima, fits return period curves, and flags concentrations exceeding a chosen return period as anomalies. EVA is applied to each sensor individually, aka each sensor has a unique anomaly threshold depending on its data provided in the time frame. "Pollution events" occur when one or more sensors are reporting anomalous measurements. Sensors participate in an event if they start reporting anomalies around the same time (ie. S1 conc is climbing and exceeds the anomaly threshold at 1:00pm, S2 does the same thing 30 mins later ... these anomalous periods started within an hour of each other and so they are part of the same event, E1).

## Process

**Get Data:**
1. Set five parameters in config.py: start_dt, end_dt, facility_id, pollutant, return_period_days, block_width_seconds
2. Run get_sensor_ids.py -- this generated the facility_dict.json file, which is a descriptor of the facility's equipment, positions, sensors, sensitivities
   **NOTE: the lat and lon of each piece of equipment is the most recent position reported. This is not a historical position.**
3. Run get_time_series.py -- thi spopulates data/processed_samples with conc-time series data for each sensor. It only returns what is available. To avoid being overwhelmed by csv files, get_time_series.py will delete exisitng files in the   directory that are not useful. It also prints to the log if the conc values were adjusted using the sensitivity.

**Run EVA:**
* For an overlay of all sensors and their anomalous periods, run compare_sensors.py
* To run EVA on an individual sensor, specifiy the equipment_id in config, then run EVA.py
* In addition to an anomlay overlay, EVA.py shows the block maxima plots and return period plots.
**NOTE: the plot generated in compare_sensor.py is automatically saved to the plots directory (plots/pollutant/facility_id/start_dt_end_dt ...). EVA.py does not have this automatic save feature implemented**

---

## Files & Responsibilities Overview

* **`EVA.py`** — For a single sensor. Computes block maxima, fits return periods, derives anomaly threshold, labels anomalies, and generates plots. Output is visual confirmation of anomalous events.

* **`CompareSensorsEVA.py`** — Runs EVA on every sensor and overlays results. Grouping logic is implemented immediately after this -- extracts anomalous segments, groups them into events, and exports `grouped_events.pkl`. This .pkl will be used in part two for the surface analysis.

* **`analysis_utils.py`** — Core analytical functions:
  * `compute_block_maxima`, `assign_return_period_labels`, `get_anomaly_threshold`, `label_anomalies`.
  * `extract_anomalous_segments`: isolates continuous anomalous runs (≥30 min by default).
  * `assign_event_groups`: merges segments from multiple sensors into events if within a 1 h response window.
  * Utilities for rolling averages, normalization, lat/lon conversion, and spatiotemporal alignment.

* **`plot_utils.py`** — Plotting library:
  * `plot_block_maxima`, `plot_return_period`, `plot_anomaly_regions` (single sensor).
  * `plot_overlay_all_sensors`, `plot_overlay_anomalies_for_all_sensors` (multi-sensor).
  * Internal gridline formatting for clean time axes.

---

# PART 2 — Spatiotemporal Source Localization

This part of the project takes the **pollution events** identified in Part 1 and estimates their likely **source origin** by analyzing the spatiotemporal spread of concentration peaks across the sensor network.  

The underlying assumption is that pollutants disperse across space and time in a way that can be represented as a surface of **arrival times**. By interpolating this surface and analyzing its gradients, we can estimate whether the event is **local** (steep surface, localized minimum within the network) or **regional** (smooth surface, gradual slope across the network).  

The workflow combines **event data (peaks or anomaly groupings), surface interpolation, and multistart gradient descent**. Results are visualized with contour plots, slope shading, and descent paths to make the origin inference interpretable.


## Process

**Input Data:**
- `surface_analysis.py` works with **either**:
  * `grouped_events.pkl` — generated by **CompareSensorsEVA.py**, where events are defined by anomalies grouped across sensors.  
  * `arrival_times_object.pkl` — generated by **polynomial_fit.py**, where events are defined by fitted peaks on sensor time series.  
- Both files contain per-event sensor data with timestamps and values. These are normalized into lists of `(sensor_id, (peak_value, peak_time))` which become the basis of the surface.

**Run Surface Analysis:**
1. **Load event data** — For each event, extract peak times per sensor. Events with fewer than 3 usable sensors are skipped (cannot fit a surface).  
2. **Convert to coordinates** — Sensor lat/lon positions are converted into a relative x–y plane (meters) using the first sensor as a reference point. Times are shifted so that the earliest arrival = 0 seconds.  
3. **Build interpolated surface** — A continuous surface of arrival times is created using cubic interpolation with a linear fallback inside the sensor convex hull. Outside the hull, values remain undefined (NaN).  
4. **Gradient descent search** — Multiple random start points inside the hull are run downhill across the arrival-time surface. Paths converge towards local minima that represent potential source origins. The best minimum (lowest arrival time) is chosen as the candidate origin.  
5. **Visualization & export** — For each event:
   - Contour plot of arrival times.  
   - Sensor locations labeled with IDs.  
   - Gradient descent paths overlaid, with the best minimum circled.  
   - Results are written to `estimated_origins.csv` with origin lat/lon, event ID, and metadata.  

**Interpretation:**
- **Local event** → Surface has strong gradients, multiple sensors reporting sharp changes, and a clear minimum inside the network.  
- **Regional event** → Surface is flatter, sloping in a consistent direction, with the minimum often outside the network.  
- **Uncertain event** → If gradients are weak or inconsistent, the estimated origin may not be reliable; surface metrics (slope magnitude, contour shapes) should be checked.

---

## Files & Responsibilities Overview

* **`surface_analysis.py`** — Main entry point for spatiotemporal analysis:
  * Accepts event data from `grouped_events.pkl` (anomalies) or `arrival_times_object.pkl` (polynomial peaks).  
  * Normalizes sensor event data into arrival times.  
  * Builds interpolated arrival-time surfaces (cubic with linear fallback).  
  * Runs multistart gradient descent across the surface.  
  * Generates per-event plots and updates `estimated_origins.csv`.  

* **`compare_sensors.py`** (Part 1) — Upstream generator of `grouped_events.pkl`. Groups anomalous sensor segments into events.  

* **`polynomial_fit.py`** — Alternative generator of `arrival_times_object.pkl`. Fits polynomial curves to each sensor segment, extracts peaks, and exports event-level peak times.  

* **`plot_utils.py`** (shared) — Provides reusable plotting functions:
  * `plot_overlay_anomalies_for_all_sensors` (used in Part 1).  
  * `plot_gradient_descent_surface` (used in Part 2 for contour + gradient descent overlays).  

---

## Notes on Use

- At least **3 sensors** are required to form a valid surface. Events with fewer are skipped.  
- Arrival-time estimation depends on method:
  * `"poly_peak"` (default) — fits a polynomial curve to identify peak time.  
  * `"peak"` — uses raw maximum concentration time.  
  * `"start"` — uses the first timestamp of anomaly detection.  
- Surfaces are constrained to the convex hull of sensor locations; origins outside this hull are not directly resolvable.  
- Gradient descent is multistart (30 runs by default) to avoid local traps and select the best global minimum.  

